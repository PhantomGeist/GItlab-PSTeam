# frozen_string_literal: true

# Authorisation is checked in the resolver.
# rubocop: disable Graphql/AuthorizeTypes

# rubocop: disable Style/ClassAndModuleChildren
module Types
  module Ai
    module Prompt
      class ExplainVulnerabilityPromptType::PresubmissionCheckResultsType < AiPromptType
        graphql_name 'ExplainVulnerabilityPresubmissionCheckResults'

        field :potential_secrets_in_code, GraphQL::Types::Boolean,
          null: false,
          description: 'This flag is true if we think there might be a ' \
                       'secret in the code that would be sent in the LLM prompt.'
        field :secret_detection_result, GraphQL::Types::Boolean,
          null: false,
          description: 'This flag is true if the vulnerability being explained ' \
                       'is specifically a secret detection vulnerability'
      end
    end
  end
end

# rubocop: enable Graphql/AuthorizeTypes
# rubocop: enable Style/ClassAndModuleChildren
