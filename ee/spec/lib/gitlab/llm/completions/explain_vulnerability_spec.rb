# frozen_string_literal: true

require 'spec_helper'

RSpec.describe Gitlab::Llm::Completions::ExplainVulnerability, feature_category: :vulnerability_management do
  using RSpec::Parameterized::TableSyntax

  let(:prompt_class) { Gitlab::Llm::Templates::ExplainVulnerability }
  let(:example_answer) { "Sure, ..." }
  let(:example_response) do
    {
      Gitlab::Llm::VertexAi::Client => {
        "predictions" => [
          {
            "candidates" => [
              {
                "author" => "",
                "content" => example_answer
              }
            ],
            "safetyAttributes" => {
              "categories" => ["Violent"],
              "scores" => [0.4000000059604645],
              "blocked" => false
            }
          }
        ],
        "deployedModelId" => "1",
        "model" => "projects/1/locations/us-central1/models/codechat-bison-001",
        "modelDisplayName" => "codechat-bison-001",
        "modelVersionId" => "1"
      }.to_json,
      Gitlab::Llm::Anthropic::Client => example_answer
    }
  end

  let(:error_response) do
    {
      Gitlab::Llm::VertexAi::Client => { error: { message: 'Ooops...' } }.to_json,
      Gitlab::Llm::Anthropic::Client => nil
    }
  end

  let(:errors) do
    {
      Gitlab::Llm::VertexAi::Client => ["Ooops..."],
      Gitlab::Llm::Anthropic::Client => ["The response from the AI provider was empty."]
    }
  end

  let_it_be(:user) { create(:user) }
  let_it_be(:user2) { create(:user) }
  let_it_be(:project) do
    create(:project, :custom_repo, files: {
      'main.c' => "#include <stdio.h>\n\nint main() { printf(\"hello, world!\"); }"
    })
  end

  let_it_be(:vulnerability) { create(:vulnerability, :with_finding, project: project) }

  let(:prompt_message) do
    build(:ai_message, :explain_vulnerability, user: user, resource: vulnerability, request_id: 'uuid')
  end

  let(:options) { {} }

  subject(:explain) { described_class.new(prompt_message, prompt_class, options) }

  def execute_explain(message_params = {}, options = {})
    message = build(:ai_message, :explain_vulnerability,
      { user: user, resource: vulnerability, request_id: 'uuid' }.merge(message_params))

    described_class.new(message, prompt_class, options).execute
  end

  before do
    allow(GraphqlTriggers).to receive(:ai_completion_response)
    vulnerability.finding.location['file'] = 'main.c'
    vulnerability.finding.location['start_line'] = 1
  end

  context 'when a null prompt is returned by the template class' do
    let(:options) { { include_source_code: true } }

    before do
      allow_next_instance_of(prompt_class) do |prompt_class|
        allow(prompt_class).to receive(:to_prompt).with(include_source_code: true).and_return(nil)
      end
    end

    it 'returns the default error response' do
      expect(GraphqlTriggers).to receive(:ai_completion_response).with(
        an_object_having_attributes(
          content: '',
          role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
          request_id: 'uuid',
          errors: [described_class::NULL_PROMPT_ERROR]
        )
      )

      explain.execute
    end
  end

  shared_examples 'explain vulnerability completions' do |llm_client, client_method|
    context 'when the client returns an unsuccessful response' do
      before do
        allow_next_instance_of(llm_client) do |client|
          allow(client).to receive(client_method).and_return(
            error_response[llm_client]
          )
        end
      end

      it 'publishes the error to the graphql subscription' do
        explain.execute

        expect(GraphqlTriggers).to have_received(:ai_completion_response)
          .with(an_object_having_attributes(
            user: user,
            resource: vulnerability,
            content: '',
            role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
            request_id: 'uuid',
            errors: errors[llm_client]
          ))
      end
    end

    context 'when the client returns a successful response' do
      before do
        allow(llm_client).to receive(:new).and_call_original
        allow_next_instance_of(llm_client) do |client|
          allow(client).to receive(client_method).and_return(example_response[llm_client])
        end
      end

      it 'sets the vertex client to retry content blocked requests' do
        explain.execute

        expect(llm_client).to have_received(:new).with(user, **extra_arguments)
      end

      it 'publishes the content from the AI response' do
        explain.execute

        expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
          an_object_having_attributes(
            content: example_answer,
            role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
            request_id: 'uuid',
            errors: [],
            user: user,
            resource: vulnerability
          )
        )
      end

      context 'when an unexpected error is raised' do
        let(:error) { StandardError.new("Ooops...") }

        before do
          allow_next_instance_of(llm_client) do |client|
            allow(client).to receive(client_method).and_raise(error)
          end
          allow(Gitlab::ErrorTracking).to receive(:track_exception)
        end

        it 'records the error' do
          explain.execute
          expect(Gitlab::ErrorTracking).to have_received(:track_exception).with(error)
        end

        it 'publishes a generic error to the graphql subscription' do
          explain.execute

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              content: '',
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [described_class::DEFAULT_ERROR],
              user: user,
              resource: vulnerability
            ))
        end
      end

      context 'when the client experiences a Net::ReadTimeout' do
        let(:error) { Net::ReadTimeout.new }

        before do
          allow_next_instance_of(llm_client) do |client|
            allow(client).to receive(client_method).and_raise(error)
          end
          allow(Gitlab::ErrorTracking).to receive(:track_exception)
        end

        it 'records the error' do
          explain.execute
          expect(Gitlab::ErrorTracking).to have_received(:track_exception).with(error)
        end

        it 'publishes a generic error to the graphql subscription' do
          explain.execute

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              content: '',
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [described_class::CLIENT_TIMEOUT_ERROR],
              user: user,
              resource: vulnerability
            ))
        end
      end

      context 'when request is cached', :use_clean_rails_redis_caching do
        context 'when `include_source_code` is not used' do
          before do
            allow_next_instance_of(llm_client) do |client|
              allow(client).to receive(client_method).once.and_return(example_response[llm_client])
            end

            explain.execute
          end

          it 'executes the request just once' do
            expect(llm_client).not_to receive(:new)

            explain.execute

            expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
              an_object_having_attributes(
                content: example_answer,
                role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
                request_id: 'uuid',
                errors: [],
                user: user,
                resource: vulnerability
              )).twice
          end
        end

        context 'when the `include_source_code` option is passed' do
          where(:include_source_code) do
            [
              true,
              false,
              nil
            ]
          end

          with_them do
            let(:options) { { include_source_code: include_source_code } }

            before do
              allow_next_instance_of(llm_client) do |client|
                allow(client).to receive(client_method).once.and_return(example_response[llm_client].to_json)
              end
            end

            it 'passes the parameter to the template class appropriately' do
              expect_next_instance_of(prompt_class) do |instance|
                expect(instance).to receive(:to_prompt)
                                      .with(include_source_code: include_source_code)
                                      .and_call_original
              end

              explain.execute
            end
          end
        end

        context 'when the `include_source_code` option is toggled' do
          before do
            allow_next_instance_of(llm_client) do |client|
              allow(client).to receive(client_method).twice.and_return(example_response[llm_client])
            end
          end

          it 'bypasses the cache and make a fresh request' do
            # cache miss
            execute_explain({}, { include_source_code: true })
            execute_explain({}, { include_source_code: false })
            execute_explain({}, {})

            # cache hit
            execute_explain({}, { include_source_code: true })
            execute_explain({}, { include_source_code: false })
            execute_explain({}, {})

            expect(GraphqlTriggers).to have_received(:ai_completion_response)
              .with(an_object_having_attributes(
                user: user,
                resource: vulnerability,
                content: example_answer,
                role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
                request_id: 'uuid',
                errors: []
              )).exactly(6).times
          end
        end

        context 'when unique users make the same request' do
          let(:fake_client) { instance_double(llm_client) }

          before do
            allow(llm_client).to receive(:new).and_return(fake_client)
            allow(fake_client).to receive(client_method).twice.and_return(example_response[llm_client])
          end

          it 'makes a fresh request for each user' do
            # cache miss
            execute_explain
            execute_explain({ user: user2 })

            # cache hit
            execute_explain
            execute_explain({ user: user2 })

            expect(fake_client).to have_received(client_method).exactly(2).times
          end
        end
      end
    end
  end

  describe '#execute', :clean_gitlab_redis_cache do
    let(:tracking_context) { { request_id: "uuid", action: :explain_vulnerability } }

    context 'when the explain_vulnerability_anthropic feature flag is enabled' do
      it_behaves_like 'explain vulnerability completions', Gitlab::Llm::Anthropic::Client, :stream do
        let(:extra_arguments) { { tracking_context: tracking_context } }
      end
    end

    context 'when the explain_vulnerability_anthropic feature flag is disabled' do
      before do
        stub_feature_flags(explain_vulnerability_anthropic: false)
      end

      it_behaves_like 'explain vulnerability completions', Gitlab::Llm::VertexAi::Client, :text do
        let(:extra_arguments) { { retry_content_blocked_requests: true, tracking_context: tracking_context } }
      end
    end
  end
end
