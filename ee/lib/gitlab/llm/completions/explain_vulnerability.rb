# frozen_string_literal: true

module Gitlab
  module Llm
    module Completions
      class ExplainVulnerability < Gitlab::Llm::Completions::Base
        DEFAULT_ERROR = 'An unexpected error has occurred.'
        CLIENT_TIMEOUT_ERROR = 'The upstream AI provider request timed out without responding.'
        NULL_PROMPT_ERROR = 'Empty prompt error. The vulnerability may be ' \
                            'incomptible with the `include_source_code` parameter.'

        def execute
          response = response_for(user, vulnerability, options)
          response_modifier = modify_response(response, vulnerability)
          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        rescue StandardError => error
          Gitlab::ErrorTracking.track_exception(error)

          response = formatted_error_response(error_message(error))
          response_modifier = modify_response(response, vulnerability)
          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        end

        private

        def error_message(error)
          case error
          when Net::ReadTimeout
            CLIENT_TIMEOUT_ERROR
          else
            DEFAULT_ERROR
          end
        end

        def formatted_error_response(message)
          { error: { message: message } }.to_json
        end

        def response_for(user, vulnerability, options)
          Rails.cache.fetch(cache_key(user, vulnerability, options), expires_in: 5.minutes, skip_nil: true) do
            prompt = ai_prompt_class.new(vulnerability, options).to_prompt(
              include_source_code: options[:include_source_code]
            )

            if prompt.to_s.empty?
              formatted_error_response(NULL_PROMPT_ERROR)
            else
              request(user, prompt, vulnerability)
            end
          end
        end

        # There is some code duplication for the evaluation for different models.
        # While it would be cleaner to seperate these, once a model is settled upon, additional files
        # will be redundant and deleted. This is a lower effort approach for code that will ultimately
        # be disposed of.

        def modify_response(response, vulnerability)
          if anthropic?(vulnerability)
            ::Gitlab::Llm::Anthropic::ResponseModifiers::ExplainVulnerability.new(response)
          else
            ::Gitlab::Llm::VertexAi::ResponseModifiers::Predictions.new(response)
          end
        end

        def request(user, prompt, vulnerability)
          if anthropic?(vulnerability)
            client_class = ::Gitlab::Llm::Anthropic::Client
            {
              completion: client_class
                            .new(user, tracking_context: tracking_context)
                            .stream(prompt: "\n\nHuman: #{prompt}\n\nAssistant:")
            }.to_json
          else
            client_class = ::Gitlab::Llm::VertexAi::Client
            client_class
              .new(user, retry_content_blocked_requests: true, tracking_context: tracking_context)
              .text(content: prompt)
          end
        end

        def anthropic?(vulnerability)
          Feature.enabled?(:explain_vulnerability_anthropic, vulnerability.project)
        end

        def cache_key(user, vulnerability, options)
          include_source_code_key = "include_source_code:#{options[:include_source_code].inspect}"

          [user.id, vulnerability.cache_key, 'explain', include_source_code_key].join('/')
        end

        def vulnerability
          resource
        end
      end
    end
  end
end
